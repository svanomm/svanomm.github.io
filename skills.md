---
layout: page
title: Skills & Technologies
permalink: /skills/
---

# Technical Skills & Expertise

My technical toolkit spans programming languages, statistical software, and specialized libraries for economics, machine learning, and data science applications.

## Programming Languages

### Python
**Advanced Proficiency** - Primary language for research and development

I have extensive experience with Python across multiple domains, from academic research to practical applications. My Python work demonstrates proficiency in:
- Advanced data analysis and statistical modeling
- Machine learning and deep learning implementations  
- Real-time data processing and API integration
- Research-grade code with reproducible results

### R  
**Advanced Proficiency** - Statistical analysis and econometric modeling

Deep expertise in R for economic research and statistical analysis:
- Econometric modeling and time series analysis
- Advanced statistical methods and hypothesis testing
- Data visualization and reporting
- Academic research and publication-quality analysis

### Stata
**Proficient** - Econometric analysis and research replication

Solid foundation in Stata for:
- Applied econometric analysis
- Research replication and robustness checks
- Academic research standards
- Policy evaluation and causal inference

### LaTeX
**Proficient** - Academic writing and document preparation

Experience with LaTeX for:
- Academic paper preparation
- Research documentation
- Professional presentations
- Mathematical notation and formatting

### SQL
**Intermediate** - Database management and data extraction

Working knowledge of SQL for:
- Database querying and data extraction
- Data pipeline development
- Business intelligence applications

---

## Python Libraries & Frameworks

### Machine Learning & Deep Learning

**TensorFlow & Keras**
- Neural network architectures for time series and NLP
- Real-time prediction systems
- Advanced model optimization and tuning

**PyTorch** 
- Deep learning research and experimentation
- Custom model development
- GPU-accelerated computing

**scikit-learn**
- Classical machine learning algorithms
- Model evaluation and cross-validation
- Feature engineering and selection

**LightGBM**
- Gradient boosting for structured data
- High-performance predictive modeling
- Ensemble method implementations

**statsmodels**
- Statistical modeling and econometrics
- Time series analysis and forecasting
- Hypothesis testing and inference

### Natural Language Processing

**Transformers & Hugging Face**
- State-of-the-art language models
- Text classification and sentiment analysis
- Custom NLP pipeline development

**sentence-transformers**
- Semantic text embeddings
- Document similarity and clustering
- Information retrieval applications

**Advanced Text Processing**
- tiktoken, tokenizers for text preprocessing
- Large-scale text analysis
- Real-time text processing pipelines

### Data Analysis & Visualization

**pandas**
- Advanced data manipulation and analysis
- Time series data processing
- Large dataset handling and optimization

**NumPy**
- Numerical computing and linear algebra
- Array operations and mathematical functions
- Performance optimization

**polars**
- High-performance data processing
- Modern DataFrame operations
- Memory-efficient data analysis

**matplotlib & seaborn**
- Statistical data visualization
- Publication-quality figures
- Interactive plotting and dashboards

### Specialized Libraries

**Financial & Economic Data**
- Real-time financial data processing
- Economic time series analysis
- Market data integration and analysis

**Research & Academic Tools**
- Jupyter ecosystem for research notebooks
- Academic-standard documentation
- Reproducible research workflows

---

## Research & Analytical Skills

### Econometric Methods
- **Causal Inference**: Difference-in-differences, instrumental variables
- **Time Series**: ARIMA, VAR models, cointegration analysis  
- **Cross-sectional**: Regression analysis, matching methods
- **Panel Data**: Fixed/random effects, dynamic panel models

### Machine Learning Applications
- **Predictive Modeling**: Classification, regression, forecasting
- **Feature Engineering**: Domain-specific feature creation
- **Model Evaluation**: Cross-validation, performance metrics
- **Ensemble Methods**: Model combination and optimization

### Data Science Workflows
- **Data Pipeline Development**: ETL processes, automation
- **Real-time Processing**: Streaming data and live analysis
- **Business Intelligence**: Dashboard creation, reporting
- **Research Infrastructure**: Reproducible analysis frameworks

---

## Domain Expertise

### Financial Markets
- Stock price prediction and analysis
- Real-time sentiment analysis for trading
- Risk modeling and portfolio optimization
- Market microstructure analysis

### Applied Economics
- Labor market analysis and policy evaluation
- Development economics and growth modeling
- Health economics and impact assessment
- Public policy analysis and causal inference

### Data Science Applications
- Large-scale text processing and NLP
- Business intelligence and analytics
- Predictive modeling for decision support
- Research tool development

---

## Software Development Practices

### Version Control & Collaboration
- Git/GitHub for code management
- Open-source project contribution
- Collaborative research workflows
- Code review and documentation standards

### Research Standards
- Reproducible research practices
- Academic-quality code documentation
- Statistical software best practices
- Data management and archiving

### Performance Optimization
- GPU computing for deep learning
- Memory-efficient data processing
- Parallel computing and optimization
- Scalable algorithm implementation

---

## Continuous Learning

I stay current with developments in:
- **Econometric Methods**: New causal inference techniques
- **Machine Learning**: Latest architectures and approaches
- **Data Science Tools**: Emerging libraries and frameworks
- **Research Methods**: Best practices in reproducible research

My technical skills continue to evolve through active research, practical applications, and engagement with the broader academic and professional community.